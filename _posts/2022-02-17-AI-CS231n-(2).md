---
published: true
title: '[인공지능] AI - CS231n (2) Loss Functions and Optimization'
layout: post
subtitle: 'csReview AI cs231n'
categories: csReview
tags: AI
comments: true
---

### AI 기초지식 쌓기
저번 시간에는 간단하게 왜 이미지를 인식하는 것이 어려운 지 살펴보았고, 이를 해결하기 위해 data-driven approach와 분류를 위한 KNN 알고리즘, 선형 분류기 등을 살펴보았다. 오늘 살펴볼 내용은 얼마나 학습 함수가 구린지 알 수 있는 loss function과 이를 최소화하는 Optimization에 대해 배워볼 예정이다.
**덧붙인 사진과 code는 모두 cs231n의 강의자료를 참고했습니다.**  

[cs231n 3강 youtube 강의 링크](https://www.youtube.com/watch?v=h7iBpEHGVNc&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=3)  
[cs231n 공개 강의자료](http://cs231n.stanford.edu/slides/)  
[cs231n 과제 링크](https://cs231n.github.io/assignments2021/assignment1/#setup)  

---
# CS231n 3강 - Loss Functions and Optimization

## Keywords

- Loss function
- Multiclass SVM loss(Hinge loss)
- Data Loss
- Regularization (L1 & L2), Weight Decay
- Softmax Classifier(Multinomial Logistic Regression)
- gradient descent
- Stochastic Gradient Descent(SGD)
- Histogram of Oriented Gradients(HoG)
- Bag of Words

---

## 1. Loss function
loss function(손실 함수)이란 모델의 성능을 나타내는 데 사용되는 함수로 정확히는 얼마나 나쁜지를 알려준다. 이 값이 클수록 모델의 성능이 낮음을 의미한다. 따라서, 손실 함수의 값이 작아지도록 모델을 학습해야 한다. 데이터셋이 {(xi, yi)} (i=1부터 N까지)일 때, x<sub>i</sub>가 이미지이고 y<sub>i</sub>가 레이블(x에 대한)이라면 loss는 다음과 같이 계산해볼 수 있다.  
![loss 계산](https://sundongkim-dev.github.io/assets/img/AI/loss.png)  
위 식을 풀어보면, x와 W를 통해 얻은 예측값과, 실제 정답인 y를 가지고 손실값을 구한다. 모든 데이터에 대해 반복하여 얻은 결과의 평균을 구한 것이 loss이다.

### 1. Multiclass SVM loss  
(x<sub>i</sub>,y<sub>i</sub>)에 대해 얻은 결과인 score vector를 s라고 하자면 SVM loss는 다음과 같다. s<sub>j</sub>는 j번 째 클래스에 대한 점수이고, s<sub>y<sub>i</sub></sub>는 정답 class에 대한 예측 값이다.  
![SVMloss 계산](https://sundongkim-dev.github.io/assets/img/AI/SVMloss.png)  
정답 class에 대한 예측값(s<sub>y<sub>i</sub></sub>)이 오답 class에 대한 예측값(s<sub>j</sub>)에 1을 더한 것보다 크면 0이 된다. 즉, 정답 class에 대한 예측을 오답 class들에 대한 예측에 비해 더 잘했다면(여기서는 그 기준이 1이며 이를 safty margin이라고 한다) loss를 0으로 간주한다는 것이다. 이를 그래프로 표현하면 아래와 같은데 마치 hinge처럼 생겨서 "hinge loss"라고도 한다.  

또한 상대적으로 값을 구하기 때문에 정답 클래스가 다른 클래스보다 높냐를 중히 여긴다. 즉, 답의 점수가 어떤가는 따지지 않는다.  
![Hinge loss](https://sundongkim-dev.github.io/assets/img/AI/Hingeloss.png)

![SVM loss Example](https://sundongkim-dev.github.io/assets/img/AI/Svmloss-Example.png)
위 사진에서 SVM loss를 구해보자. 고양이의 예시만 구해보면 아래와 같다.
```
max(0, 5.1 - 3.2 + 1) + max(0, -1.7 - 3.2 + 1)
= max(0, 2.9) + max(0, -3.9) = 2.9 + 0
= 2.9 (첫 번째 고양이 사진에 대한 loss)
```
같은 방식으로 구하면 각각 2.9, 0, 12.9이다. 이의 평균을 구하면 곧 전체 데이터셋에 대한 loss가 된다.
> L = (2.9 + 0 + 12.9) / 3 = 5.27

Q1. loss값의 최저/최대값은?
> 최저는 0이며 최대는 이론상 무한이다.

Q2. 일반적으로 가중치 W를 매우 작은 값으로 초기화하는데, 모든 s는 0에 가깝다. 이때 Loss 값은?
(class 수 - 1)이다. 전부 max(0, 0-0+1)과 같은 결과를 뱉기 때문이다.

Q3. 정답 클래스 경우를 loss구할 때 포함하면?
max(0, val-val+1)과 같은 결과를 뱉기에 그렇지 않은 경우와 1만큼 차이난다. ~~왜 이렇게? 1보단 0이 기준 세우기 좋아서~~  



## 2. Regularization  


## 3. Softmax Classifier(Multinomial Logistic Regression)


## 4. Optimization


1. Random Search
2. Follow the slope

Numerical gradient
Analytic gradient
gradient check

Gradient Descent
Stochastic Gradient Descent(SGD)


## 5. Image Features
1. Motivation
2. Color Histogram
3. Histogram of Oriented Gradients (HoG)
4. Bag of Words

## 6. Image features vs ConvNets
