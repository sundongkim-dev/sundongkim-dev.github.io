---
published: true
title: '[인공지능] AI - CS231n 9강 CNN Architectures'
layout: post
subtitle: 'csReview AI cs231n'
categories: csReview
tags: AI
comments: true
---

[cs231n 9강 youtube 강의 링크](https://www.youtube.com/watch?v=DAOcjicFr1Y)  
[cs231n 공개 강의자료](http://cs231n.stanford.edu/slides/)  
[cs231n 과제 링크](https://cs231n.github.io/assignments2021/assignment1/#setup)  

---
# CS231n 9강 - CNN Architectures

## Keywords
- AlexNet
- VGG
- Receptive field
- GoogLeNet
- Inception module
- ResNet
- Residual block
- Network in Network(NiN)
- Wide ResNet
- ResNeXt
- Stochastic Depth
- FractalNet
- DenseNet
- SqueezeNet

---

이번 강의에서는 최신 ~~2017년 기준..~~ CNN 아키텍쳐들에 대해 배워볼 시간이다. 사람들이 어떤 식으로 CNN을 구성했는지 확인해보자.

## 1. AlexNet
최초의 large scale CNN으로 2012년 당시의 딥러닝이 아닌 모델들을 능가하는 놀라운 성능을 보여줬다. layer 구조는 LeNet과 상당히 유사하다. LeNet의 경우 [Conv-Pool-Conv-Pool-Fc-Fc]이고 AlexNet은 아래와 같다.  
![AlexNet](https://sundongkim-dev.github.io/assets/img/AI/AlexNet.png)

Q1. AlexNet의 입력의 크기가 227x227x3이고, CONV1에서 11x11 필터가 stride=4로 96개가 있다면 CONV1의 출력 사이즈는?
> (227-11)/4+1 = 55이므로 55x55x96이다. 답을 모르겠다면, [여기로](https://sundongkim-dev.github.io/csreview/2022/02/22/AI-CS231n-(4))

Q2. CONV1의 총 파라미터 수는?
> 11*11*3*96 = 34848 개이다.

Q3. POOL1에서 3x3 필터가 stride=2라면 POOL1의 출력 사이즈는?
> (55-3)/2+1 = 27이므로 27x27x96이다.

Q4. POOL1의 총 파라미터 개수는?
> 0개이다!! 이전 layer에서 Maxpooling으로 값을 추리는 것이기 때문에 파라미터가 없다.

AlexNet은 ReLU를 처음으로 사용했으며, 지금은 잘 사용하지 않는 Norm layers들도 사용했다. 또한 위 그림을 보면 알겠지만, 하나의 Input 데이터가 둘로 나누어져서 학습이 이루어지고 있는데, 이는 당시에 GPU의 메모리가 충분하지 않았기 때문에 둘로 나누어서 학습해야 했기 때문이다. ~~나중에 나누어 처리하는 과정을 살펴보자~~ 그렇기에 CONV1,2,4,5에서는 서로 다른 GPU에 나눠진 feature map만을 학습하게 되므로 이를 합쳐주는 구간이 필요했다. 그런 구간이 CONV3, FC6-7-8이다.

이후에 ZFNet이 이 AlexNet의 파라미터 값만을 변경했는데도 성능이 많이 좋아졌다. CONV1에서 11x11 stride4를 7x7 stride2로, CONV3-4-5에서 필터를 각각 512, 1024, 512개를 사용하였다. 파라미터값의 설정이 얼마나 중요한 지 알 수 있는 대목이다.

## 2. VGGNet
VGGNet은 AlexNet과 뭐가 달라졌을까? 가장 큰 차이점은 layer가 깊어졌다는 것이다. AlexNet은 8개의 layer에 그치지만, VGG는 뒤에 숫자에 따라 layer가 16개에서 19개를 사용하였다. 또한 CONV에서 3x3의 아주 작은 필터를 사용하였다.

왜 이렇게 작은 필터만을 사용했을까? 3개의 3x3 conv(stride: 1)를 사용하면 7x7의 필터를 적용한 것과 같은 효과를 얻을 수 있다. 이는 Receptive field 측면에서 그런건데 아래 그림을 보면 보다 잘 이해할 수 있다.

![출처: https://medium.com/deep-learning-g/cnn-architectures-vggnet-e09d7fe79c45](https://sundongkim-dev.github.io/assets/img/AI/3x3-receptive.png)  
*출처: https://medium.com/deep-learning-g/cnn-architectures-vggnet-e09d7fe79c45*

똑같이 7x7만큼의 데이터가 있다면 7x7 필터를 적용하면 1x1의 output을 얻을 수 있고 3x3 필터 3개를 적용하면 마찬가지로 1x1의 output을 얻어 같은 결과를 얻을 수 있다.

그래서 어떤게 이득일까?  
일단 첫 번째로, 학습 파라미터 수가 감소한다. 하나의 layer 당 channel 수를 C라고 하자. 이는 곧 depth와 같다. 7x7 필터를 사용한 경우의 파라미터 수를 계산해보자. 7x7 필터에 depth를 곱해주고 input depth와 output depth를 같게 유지해야 하기 때문에 depth만큼 한 번 더 곱해주어야 한다. 결론적으로 7x7xCxC 개이다. 3x3의 경우는 같은 방식이지만 3 개의 stack을 쌓기 때문에 3을 더 곱해주면 된다. 결과적으로 3x(3x3xCxC) 개이다. 결국 27 < 49로 파라미터 수가 많이 줄어든 것을 확인할 수 있다.

두 번째로, 같은 receptive field를 유지한 채로 더 깊은 layer를 쌓을 수 있게 되어서 비선형성이 증가한다. 비선형성이 증가하면 일반적으로, 모델이 특징을 잘 식별하게 된다.  

위의 식을 토대로 파라미터의 수를 세어보면 다음과 같다.  
![VGGNet parameters](https://sundongkim-dev.github.io/assets/img/AI/counting-parameters.png)  
수치를 잘 살펴보면, 초기의 CONV layer가 많은 메모리를 잡아먹고 있!


## 3.
