---
published: true
title: '[인공지능] AI - CS231n (4) Convolutional Neural Networks'
layout: post
subtitle: 'csReview AI cs231n'
categories: csReview
tags: AI
comments: true
---

[cs231n 5강 youtube 강의 링크](https://www.youtube.com/watch?v=d14TUNcbn1k&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=4)  
[cs231n 공개 강의자료](http://cs231n.stanford.edu/slides/)  
[cs231n 과제 링크](https://cs231n.github.io/assignments2021/assignment1/#setup)  

---
# CS231n 5강 - Convolutional Neural Networks

## Keywords
- Convolutional Neural Networks
- Filter / stride / zero padding
- Activation map
- Pooling layer
- Max Pooling

---

드디어 강의 제목인 CNN에 도달했다. CNN은 간단하게 말하면 input 이미지의 일부들을 conolution하여 image map을 만들고 subsampling 한 뒤에 FC layer를 통해 출력을 만들어내는 것이다.

지난 시간엔 신경망을 살펴봤고 CNN의 역사에 대해 살짝 훑고 가자.

## 1. CNN까지의 역사
1. **MARK 1 Perceptron**(1957): 단층 퍼셉트론으로 w값을 조절해서 학습했으며 w값을 조절하는 update rule이 처음 등장했다.
2. Multi-layer Perceptron Network(1960)

이후, 처음으로 Backpropagation을 사용하는 모델이 나타났다. 하지만 암흑기가 찾아왔고 오랜 시간이 지난 2006년에 W값을 잘 주기 위한 방법이 나왔다. 그러다 2012년에 CNN 기반의 alexNet이 세상에 나오게 되었다. 이때부터 급격하게 발전하기 시작한다.


## 2. Convolutional Neural Networks

이전 시간에 우리가 배운 Fully connected layer는 그냥 image 자체를 쭉 늘려서 사용했었다. 예를 들어, image가 32 * 32 * 3의 정보를 갖는다면 3072 * 1로 사용하여 가중치 W(10*3072)와 내적하여 1 * 10의 activation layer(map)을 얻었다.

Convolutional layer는 FC layer와 어떤 차이가 있을까?

Convolutional layer는 이미지의 공간 정보를 보존한다. 공간 정보를 보존하려면 어떻게 해야할까? 우선, 공간 정보를 보존한다는 것은 activation map을 만들 때 일렬로 stretch하지 않는다는 말이다. 또한, 공간 정보를 보존하기 위해서는 filter를 사용해야 한다.

Filter는 항상 image와 depth가 일치해야 한다. 그래야 공간 정보를 다 보존할 수 있기 때문이다. 이제 해야할 일은 말 그대로 filter와 image를 convolve해야 한다. 신호 쪽에서 배운 합성곱은 하나의 함수와 또 다른 함수를 반전 이동한 값을 곱한 후, 구간에 대해 적분하여 새로운 함수를 구하는 연산자인데, 여기서는 엇비슷하다. 정확히 대응되진 않는 것 같다.

어찌됐든, 필터를 쭉 slide하면서 내적을 해주면 된다. 위의 예시에서 필터가 5X5X3이고 이를 w라고 한다면 w<sup>T</sup>x+b이다. b는 bias이고 결과는 28X28X1의 activation map이 나올 것이다. activation map의 개수는 결국 필터의 개수와 일치한다.

Filter를 슬라이딩해서 얻은 결과인 activation map의 크기는 어떻게 결정될까? Filter의 크기도 중요하지만 Filter의 간격인 stride도 중요하다. Stride에 따른 activation map을 이해하기 위해 예시를 살펴보자.

![Stride](https://sundongkim-dev.github.io/assets/img/AI/Stride-Example.PNG)  
Q1. 7X7 입력에서 3X3 필터를 사용한다고 해보자. 결과물은?
> 5X5가 출력될 것이다.

Q2. 1번과 같은 조건에서 stride가 2라면?
> 3X3이 출력될 것이다. 말그대로 간격이 2이기 때문이다. 만약 이해가 가질 않는다면 기본 슬라이딩의 stride값이 1이라는 것을 생각하자.

Q3. 1번과 같은 조건에서 이젠 stride가 3이라면?
> filter가 input에 fit하지 않기 때문에 적용 불가능하다. 적용하게하려면 다른 방식을 사용해야 한다.(zero-padding 등)

위를 토대로 공식화해보면 Output의 사이즈는 (N-F)/stride + 1임을 알 수 있다. N은 input image의 길이이며, F는 필터의 길이이다. stride가 3일 때 적용 불가한 것도 위 공식에 대입해서 계산해보면 수가 딱 떨어지지 않음을 알 수 있다.

이렇게 딱 떨어지지 않을 때 사용하는 일반적인 방법은 입력 이미지에 zero-padding을 하는 것이다. 이미지 모서리를 전부 0으로 감싸주는 것이다.  
![Zero-padding](https://sundongkim-dev.github.io/assets/img/AI/zero-padding.png)  
위와 같이 말이다. Zero-padding은 이뿐만 아니라 아주 놀라운 기능을 하나 더 가지고 있다. 바로 출력 이미지의 크기를 유지할 수 있단 점이다. 이전의 activation map은 전부 크기가 줄어들고 모서리의 정보가 누락되어왔기 때문에 이런 점은 매우 좋은 장점이다. zero-padding 덕에 마지막까지 이미지의 모든 부분을 전달할 수 있게 된 것이다.

Q1. 32x32x3이 주어지는데 2만큼 padding하고 stride 1을 쓰는 10개의 5x5 필터를 사용하면 output의 사이즈는?
> (32+2*2-5)/1 + 1 = 32이다. 10개의 필터를 사용하므로 32x32x10이다.

정보를 줄이고 싶지 않아서 zero-padding을 쓴다고 했는데 정보를 줄이고 싶다면 무엇을 쓸까? zero-padding을 쓰지 않고 stride, filter size를 적절히 조절해서 Conv layer에 통과시키는 방법도 있겠지만 일반적으로 pooling에서 이뤄진다.

또는 1x1의 convolution layer를 사용하는 방법도 있다.
![1*1 Conv layers](https://sundongkim-dev.github.io/assets/img/AI/1x1-Convolutional-Layer.png)  


![ConvNet Conv layers](https://sundongkim-dev.github.io/assets/img/AI/ConvNet-Example.png)  
위의 예시는 일반적인 ConvNet의 구조인데 input image가 Conv layer와 ReLU 활성 함수를 번갈아 통과하고 Pooling layer를 통과하고 있음을 알 수 있다.

![VGG Visualization](https://sundongkim-dev.github.io/assets/img/AI/VGG-Visualization.png)    
위의 그림은 image를 넣고 얻은 feature들을 시각화한 것인데 대체로 해석이 되는 것은 별로 없다. 그래도 알 수 있는 것은 Conv1_1에서는 low-level feature인 edges나 color를 학습하고 그 후에는 corner, blobs 등 높은 레벨의 특징을 학습하게 된다. 나중에 Visualization에 대한 여러 시도를 배우니 그때 좀 더 다뤄보자.

![CNN Car example](https://sundongkim-dev.github.io/assets/img/AI/CNN-Car-Example.png)  
위의 구조에서 우리는 Conv layer가 뭘 하는 지 알게 되었다. 이 층에서 나온 값들은 ReLU와 POOL을 거치고 이를 반복하다 FC layer를 만나서 답을 출력할 것이다.

## The brain/neuron view of CONV layers
![The brain/neuron view of CONV layers](https://sundongkim-dev.github.io/assets/img/AI/brainView-Conv-Layers.png)  
위와 같이 뉴런의 관점에서 보면 Convolutional layer는 하나의 뉴런과 같다. 결과적으로 32x32x3에서 뽑아낸 28x28은 뉴런의 출력이고 각각은 입력 이미지의 조그마한 부분과 연결되며 모든 부분은 동일한 파라미터를 공유한다. 특정 부분만 처리하는 뉴런 여러 개가 모여 전체 이미지를 보게 되는 것과 유사하다.

이러한 비유를 FC layer에도 해보면 각 뉴런이 전체 입력 이미지를 한 번에 보는 것이라고 할 수 있다.

정리하면, Conv layer는 입력 이미지를 여러 작은 부분으로 나누어 바라보고 FC layer는 전체적으로 한 번 보는 것과 같다. 결론적으로 Convolution layer는 입력 이미지 일부분에서 특징을 추출하므로 전체 이미지에서는 여러 개의 특징을 추출할 수 있게 되어 이미지를 변형(확대, 축소, 이동)해도 이미지의 특징을 잘 찾을 수 있다. 그러나 FC layer는 이미지 전체에서 특징 하나(템플릿)를 추출하므로 효과적이지 않다.
