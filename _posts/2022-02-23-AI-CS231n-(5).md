---
published: true
title: '[인공지능] AI - CS231n (5) Training Neural Networks, Part 1'
layout: post
subtitle: 'csReview AI cs231n'
categories: csReview
tags: AI
comments: true
---

[cs231n 6강 youtube 강의 링크](https://www.youtube.com/watch?v=wEoyxE0GP2M&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=6)  
[cs231n 공개 강의자료](http://cs231n.stanford.edu/slides/)  
[cs231n 과제 링크](https://cs231n.github.io/assignments2021/assignment1/#setup)  

---
# CS231n 6강 - Training Neural Networks, Part 1

## Keywords
- Activation functions(Sigmoid, tanh, ReLU, Leaky ReLU, Maxout, ELU)
- zero-centered / saturation
- Data Preprocessing
- Xavier initialization
- Batch Normalization
- Sanity check
- Weight initialization
- Regularization
- Hyperparameter Optimization
- Model ensembles

---

이제 우리는 이미지를 공간 정보를 포함하여 학습할 수 있게 되었고, 이 학습을 어떻게 시키는 가를 배울 차례이다. 그렇기에 이번엔 활성화 함수와 데이터 전처리에 대해 배울 것이다. 또한, 가중치 W를 효율적으로 초기화하는 방법과 Batch Normalization에 대해 배울 것이다.

## 1. Activation Functions
Activation function이란 input값을 다음 노드로 보낼 때 어떻게 처리해서 보낼 지를 결정해준다. 이러한 활성화 함수는 주로 비선형성을 띈 함수들이다. 그 이유는 선형함수를 사용하게 되면 여러 층을 깊게 하는 의미가 줄어들기 때문이다. 선형성이 확보된 함수들이었다면 n 개의 층을 1 개의 층으로 줄일 수 있기 때문이다.

1. Sigmoid  
![Sigmoid](https://sundongkim-dev.github.io/assets/img/AI/Sigmoid.png)
- 출력값을 0에서 1로 고정하는 함수이다.

하지만, 이 함수에는 3 가지 문제점이 있다.

첫째로, **기울기 소실 문제(gradient vanishing)**이다.   
그래프의 기울기를 보면 알겠지만 특정 구간은 기울기가 0에 가까워져 소실된다.

둘째로, **zero-centered가 아니다**. output 값이 양수, 음수 모두 가능하는 지를 말하는데, 이는 곧 파라미터들의 gradient가 서로 다른 부호를 가질 수 없다는 뜻이다.  

모두 가능하지 않다면 어떤 문제가 발생할까? 특정 벡터 공간으로 gradient가 갱신되지 않기 때문에 update가 매우 느리다. 아래 그림을 보면 알겠지만 제1사분면, 제3사분면 성분으로만 업데이트가 가능해서 지그재그로 업데이트된다. 그렇기 때문에 zero-centered, zero-mean data가 필요한 것이다.

마지막으로, **exp연산은 +,-와 달리 비싼 연산**이라는 점이다.

위 세 가지 이유로 sigmoid는 잘 사용되지 않는다.

2. tanh  
![tanh](https://sundongkim-dev.github.io/assets/img/AI/tanh.png)
- 출력값을 -1에서 1로 고정하는 함수이다.
- zero-centered하다.
- **여전히 기울기 소실 문제가 발생**한다.

그래프의 양 끝 부분을 보면 기울기가 0에 수렴하는 것을 알 수 있어 여전히 기울기 소실 문제가 발생한다는 것을 알 수 있다.

3. ReLU  
![ReLU](https://sundongkim-dev.github.io/assets/img/AI/ReLU.png)
- 출력이 양수인 부분에서는 더 이상 기울기 소실 문제가 없다.
- 연산에 있어 효율적이다.
- sigmoid나 tanh에 비해 훨씬 빨리 수렴한다. ~~약 6배나 빨리..~~
- zero-centered하지 않다.

여전히 음수인 부분에서 기울기 소실 문제가 발생한다. 예를 들어 x가 0이하인 부분에서의 기울기는 소실되는데 이를 dead ReLU라고 한다. 만약 w를 초기화 할때 이 dead ReLU에 빠져버리면 업데이트가 되지 않는다. 일부만 dead ReLU에 빠져도 학습이 잘 되지 않을 것 같지만 실제론 그렇지 않다고 한다.

4. Leaky ReLU  
![Leaky ReLU](https://sundongkim-dev.github.io/assets/img/AI/Leaky-ReLU.PNG)
- 기울기 소실 문제가 발생하지 않는다.
- 연산에 있어 효율적이다.
- sigmoid나 tanh에 비해 훨씬 빨리 수렴한다.
- dead ReLU가 없어진다.

cf) Parametric Rectifier(PReLU) =>
f(x) = max(ax, x)로, a값을 Backpropagation을 통해 결정하기 때문에 leaky ReLU보다 유연하다.

5. Exponential Linear Units (ELU)  
![ELU](https://sundongkim-dev.github.io/assets/img/AI/ELU.PNG)
- ReLU의 모든 장점을 가진다
- exp연산이 있어 연산의 cost가 비싸다.

6. Maxout  => max(w<sub>1</sub><sup>T</sup>x+b<sub>1</sub>, w<sub>2</sub><sup>T</sup>x+b<sub>2</sub>)
- ReLU와 Leaky ReLU를 일반화한 식으로 연산량이 두 배가 되어 잘 사용하지 않는다.

## 2. Data Preprocessing

## 3. Weight Initialization

## 4. Batch Normalization

## 5. Babysitting the Learning Process

## 6. Hyperparameter Optimization
